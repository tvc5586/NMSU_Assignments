{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMSU CSCI-5435 Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name:               Tianjie Chen\n",
    "#Email:              tvc5586@nmsu.edu\n",
    "#File Creation Date: Apr/2/2025\n",
    "#Purpose of File:    NMSU CSCI-5435 Assignment 5\n",
    "#Last Edit Date:     Apr/3/2025\n",
    "#Last Edit Note:     Modify text generation function\n",
    "#GenAI used:         False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING GPU\n",
    "#print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"news_summary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headlines', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   headlines        text\n",
      "0  57.427109  356.907643\n"
     ]
    }
   ],
   "source": [
    "# Get average length\n",
    "result = pd.DataFrame([[]])\n",
    "for col in df:\n",
    "    result[col] = df[col].apply(len).mean()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat text with headline\n",
    "data = []\n",
    "\n",
    "for headline, text in zip(df['headlines'], df['text']):\n",
    "    data.append(\"<start> \" + text + \" <tl;dr> \" + headline + \" <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test\n",
    "test = data[:5]\n",
    "data = data[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India. \"It would have been better if the authorities would have served the notice first if any and then publicised this,\" reads a press release issued on behalf of Rahat. The statement further called the allegation \"bizarre\". <tl;dr> Rahat Fateh Ali Khan denies getting notice for smuggling currency <end>\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, list_of_sentence, tokenization, special_token, max_tokens=None):\n",
    "        # count vocab frequency\n",
    "        vocab_freq = {}\n",
    "        tokens = tokenization(list_of_sentence)\n",
    "        for t in tokens:\n",
    "            for vocab in t:\n",
    "                if vocab not in vocab_freq:\n",
    "                    vocab_freq[vocab] = 0\n",
    "                vocab_freq[vocab] += 1\n",
    "        # sort by frequency\n",
    "        vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda i: i[1], reverse=True)}\n",
    "        # create vocab list\n",
    "        self.vocabs = special_token + list(vocab_freq.keys())\n",
    "        if max_tokens:\n",
    "            self.vocabs = self.vocabs[:max_tokens]\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def _get_tokens(self, list_of_sentence):\n",
    "        for sentence in list_of_sentence:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            yield tokens\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.vocabs\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def append_token(self, token):\n",
    "        self.vocabs.append(token)\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def __call__(self, list_of_tokens):\n",
    "        def get_token_index(token):\n",
    "            if token in self.stoi:\n",
    "                return self.stoi[token]\n",
    "            else:\n",
    "                return 0\n",
    "        return [get_token_index(t) for t in list_of_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabs)\n",
    "        \n",
    "###\n",
    "# generate Vocab\n",
    "###\n",
    "max_word = 50000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Must manually add the start and end tokens, otherwise\n",
    "# the tokenizer will separate them into three tokens\n",
    "tokenizer.add_tokens([\"<start>\", \"<end>\", \"<tl;dr>\"])\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = Vocab(\n",
    "    data,\n",
    "    tokenization=yield_tokens,\n",
    "    special_token=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "\n",
    "# get list for index-to-word, and word-to-index.\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()\n",
    "\n",
    "# Add <pad> token\n",
    "vocab.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 796, 796, 796, 796, 13, 9226, 9226, 14]\n"
     ]
    }
   ],
   "source": [
    "print(vocab(tokenizer.tokenize(\"<start> test test test test <tl;dr> ignore ignore <end>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, vocab, length=1000):\n",
    "        self.data   = data\n",
    "        self.length = length\n",
    "        self.vocab  = vocab\n",
    "        self.hashes = set()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab.__len__()\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        while True:\n",
    "            # get a random row\n",
    "            indx = np.random.randint(len(self.data))\n",
    "            \n",
    "            # get hash of random number\n",
    "            h = hash(pickle.dumps(indx))\n",
    "\n",
    "            # check if the random row is already used\n",
    "            if h not in self.hashes:\n",
    "                self.hashes.add(h)\n",
    "                break\n",
    "        \n",
    "        # tokenize to a list of word's indices\n",
    "        tokens = vocab(tokenizer.tokenize(data[indx]))\n",
    "\n",
    "        # separate into features and labels\n",
    "        stop_token = vocab(tokenizer.tokenize(\"<end>\"))[0]\n",
    "        \n",
    "        x = tokens\n",
    "        y = tokens[1:]\n",
    "\n",
    "        # limit length to max_seq_len\n",
    "        x = x[:self.length]\n",
    "        y = y[:self.length]\n",
    "        \n",
    "        # pad features and labels\n",
    "        x += [self.vocab.__len__() - 1] * (self.length - len(x))\n",
    "        y += [stop_token] * (self.length - len(y))\n",
    "\n",
    "        # convert to tensor\n",
    "        x = torch.tensor(x, dtype=torch.int64).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.int64).to(device)\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = TextDataset(data = data, vocab = vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 106.16M\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'openai-gpt'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.batch_size = 4            # default is 64\n",
    "train_config.learning_rate = 5e-5\n",
    "train_config.max_iters = 10000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 10.29015\n",
      "iter_dt 594.38ms; iter 100: train loss 0.77701\n",
      "iter_dt 601.60ms; iter 200: train loss 0.69620\n",
      "iter_dt 603.28ms; iter 300: train loss 0.68510\n",
      "iter_dt 602.63ms; iter 400: train loss 0.69366\n",
      "iter_dt 607.23ms; iter 500: train loss 0.59705\n",
      "iter_dt 604.78ms; iter 600: train loss 0.72141\n",
      "iter_dt 604.07ms; iter 700: train loss 0.72921\n",
      "iter_dt 605.06ms; iter 800: train loss 0.69311\n",
      "iter_dt 606.34ms; iter 900: train loss 0.66488\n",
      "iter_dt 608.55ms; iter 1000: train loss 0.58155\n",
      "iter_dt 606.07ms; iter 1100: train loss 0.60268\n",
      "iter_dt 607.13ms; iter 1200: train loss 0.58757\n",
      "iter_dt 606.14ms; iter 1300: train loss 0.54505\n",
      "iter_dt 608.21ms; iter 1400: train loss 0.61567\n",
      "iter_dt 608.73ms; iter 1500: train loss 0.59274\n",
      "iter_dt 607.03ms; iter 1600: train loss 0.66253\n",
      "iter_dt 605.34ms; iter 1700: train loss 0.56279\n",
      "iter_dt 606.15ms; iter 1800: train loss 0.54145\n",
      "iter_dt 603.77ms; iter 1900: train loss 0.58959\n",
      "iter_dt 604.78ms; iter 2000: train loss 0.58182\n",
      "iter_dt 606.68ms; iter 2100: train loss 0.58700\n",
      "iter_dt 604.79ms; iter 2200: train loss 0.59537\n",
      "iter_dt 604.82ms; iter 2300: train loss 0.61347\n",
      "iter_dt 604.59ms; iter 2400: train loss 0.55725\n",
      "iter_dt 603.57ms; iter 2500: train loss 0.62028\n",
      "iter_dt 603.63ms; iter 2600: train loss 0.52432\n",
      "iter_dt 606.29ms; iter 2700: train loss 0.55863\n",
      "iter_dt 605.95ms; iter 2800: train loss 0.58840\n",
      "iter_dt 607.13ms; iter 2900: train loss 0.59701\n",
      "iter_dt 606.44ms; iter 3000: train loss 0.46803\n",
      "iter_dt 600.62ms; iter 3100: train loss 0.52888\n",
      "iter_dt 604.75ms; iter 3200: train loss 0.52295\n",
      "iter_dt 604.97ms; iter 3300: train loss 0.48852\n",
      "iter_dt 603.11ms; iter 3400: train loss 0.54835\n",
      "iter_dt 605.35ms; iter 3500: train loss 0.62115\n",
      "iter_dt 603.93ms; iter 3600: train loss 0.57508\n",
      "iter_dt 606.36ms; iter 3700: train loss 0.58263\n",
      "iter_dt 602.39ms; iter 3800: train loss 0.62478\n",
      "iter_dt 604.12ms; iter 3900: train loss 0.53431\n",
      "iter_dt 603.82ms; iter 4000: train loss 0.60530\n",
      "iter_dt 605.96ms; iter 4100: train loss 0.56468\n",
      "iter_dt 604.25ms; iter 4200: train loss 0.52612\n",
      "iter_dt 606.64ms; iter 4300: train loss 0.57145\n",
      "iter_dt 602.04ms; iter 4400: train loss 0.53836\n",
      "iter_dt 605.63ms; iter 4500: train loss 0.59666\n",
      "iter_dt 602.24ms; iter 4600: train loss 0.50078\n",
      "iter_dt 602.19ms; iter 4700: train loss 0.52664\n",
      "iter_dt 607.93ms; iter 4800: train loss 0.53967\n",
      "iter_dt 608.31ms; iter 4900: train loss 0.46794\n",
      "iter_dt 604.77ms; iter 5000: train loss 0.56699\n",
      "iter_dt 605.80ms; iter 5100: train loss 0.53491\n",
      "iter_dt 605.28ms; iter 5200: train loss 0.55692\n",
      "iter_dt 603.92ms; iter 5300: train loss 0.48204\n",
      "iter_dt 610.06ms; iter 5400: train loss 0.52705\n",
      "iter_dt 605.72ms; iter 5500: train loss 0.45260\n",
      "iter_dt 602.81ms; iter 5600: train loss 0.49517\n",
      "iter_dt 603.86ms; iter 5700: train loss 0.60489\n",
      "iter_dt 604.56ms; iter 5800: train loss 0.48979\n",
      "iter_dt 605.01ms; iter 5900: train loss 0.53671\n",
      "iter_dt 604.57ms; iter 6000: train loss 0.58076\n",
      "iter_dt 603.63ms; iter 6100: train loss 0.53889\n",
      "iter_dt 609.92ms; iter 6200: train loss 0.49893\n",
      "iter_dt 607.09ms; iter 6300: train loss 0.47815\n",
      "iter_dt 606.03ms; iter 6400: train loss 0.50429\n",
      "iter_dt 607.86ms; iter 6500: train loss 0.46702\n",
      "iter_dt 602.78ms; iter 6600: train loss 0.47344\n",
      "iter_dt 603.97ms; iter 6700: train loss 0.51446\n",
      "iter_dt 604.90ms; iter 6800: train loss 0.54630\n",
      "iter_dt 608.36ms; iter 6900: train loss 0.56286\n",
      "iter_dt 606.04ms; iter 7000: train loss 0.53568\n",
      "iter_dt 604.65ms; iter 7100: train loss 0.54030\n",
      "iter_dt 605.42ms; iter 7200: train loss 0.49273\n",
      "iter_dt 604.23ms; iter 7300: train loss 0.47372\n",
      "iter_dt 606.02ms; iter 7400: train loss 0.55824\n",
      "iter_dt 603.30ms; iter 7500: train loss 0.53417\n",
      "iter_dt 603.40ms; iter 7600: train loss 0.44089\n",
      "iter_dt 605.42ms; iter 7700: train loss 0.58839\n",
      "iter_dt 606.61ms; iter 7800: train loss 0.51777\n",
      "iter_dt 600.54ms; iter 7900: train loss 0.49433\n",
      "iter_dt 606.56ms; iter 8000: train loss 0.51019\n",
      "iter_dt 601.67ms; iter 8100: train loss 0.49804\n",
      "iter_dt 602.90ms; iter 8200: train loss 0.49247\n",
      "iter_dt 606.22ms; iter 8300: train loss 0.51194\n",
      "iter_dt 606.24ms; iter 8400: train loss 0.50264\n",
      "iter_dt 602.95ms; iter 8500: train loss 0.52877\n",
      "iter_dt 603.70ms; iter 8600: train loss 0.47472\n",
      "iter_dt 601.58ms; iter 8700: train loss 0.47099\n",
      "iter_dt 604.06ms; iter 8800: train loss 0.51914\n",
      "iter_dt 607.40ms; iter 8900: train loss 0.47833\n",
      "iter_dt 602.86ms; iter 9000: train loss 0.48746\n",
      "iter_dt 603.27ms; iter 9100: train loss 0.52208\n",
      "iter_dt 604.43ms; iter 9200: train loss 0.46087\n",
      "iter_dt 603.42ms; iter 9300: train loss 0.47923\n",
      "iter_dt 607.75ms; iter 9400: train loss 0.50716\n",
      "iter_dt 604.06ms; iter 9500: train loss 0.48490\n",
      "iter_dt 605.46ms; iter 9600: train loss 0.46415\n",
      "iter_dt 609.45ms; iter 9700: train loss 0.54021\n",
      "iter_dt 606.47ms; iter 9800: train loss 0.53947\n",
      "iter_dt 604.71ms; iter 9900: train loss 0.45159\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr_index = stoi[\"<tl;dr>\"]\n",
    "\n",
    "def generate(vocab, tokenizer, prompt, num_samples=1, steps=50, do_sample=True):\n",
    "\n",
    "    test_seq  = vocab(tokenizer.tokenize(prompt))\n",
    "    input_seq = torch.tensor([test_seq[: test_seq.index(tldr_index) + 1]], dtype=torch.int64).to(device)\n",
    "    \n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(input_seq, max_new_tokens=steps, do_sample=do_sample, top_k=10)\n",
    "\n",
    "    output = \"\"\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for j in y[i]:\n",
    "            output += itos[j] + \" \"\n",
    "        output += \"\\n\"\n",
    "\n",
    "    output = output.replace(\" ##\", \"\")\n",
    "    headline = output.split(\"<tl;dr>\")[1].split(\"<end>\", 1)[0]\n",
    "    text = output.split(\"<tl;dr>\")[0].split(\"<start>\")[1]\n",
    "\n",
    "    print(f\"Text: {text}\\n\\nHeadline: {headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Saurav Kant , an alumnus of upGrad and IIIT - B ' s PG Program in Machine learning and Artificial Intelligence , was a Sr Systems Engineer at Infosys with almost 5 years of work experience . The program and upGrad ' s 360 - degree career support helped him transition to a Data Scientist at Tech Mahindra with 90 % salary hike . upGrad ' s Online Power Learning has powered 3 lakh + careers . \n",
      "\n",
      "Headline:  Manj Mahaan PNo ' scam ' luru ' s death , PNB \n"
     ]
    }
   ],
   "source": [
    "generate(vocab, tokenizer, prompt=test[0], num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Kunal Shah ' s credit card bill payment platform , CRED , gave users a chance to win free food from Swiggy for one year . Pranav Kaushik , a Delhi techie , bagged this reward after spending 2000 CRED coins . Users get one CRED coin per rupee of bill paid , which can be used to avail rewards from brands like Ixigo , BookMyShow , UberEats , Cult . Fit and more . \n",
      "\n",
      "Headline:  I - in Ideal Bank to invests I - in Uj ' s Uber \n"
     ]
    }
   ],
   "source": [
    "generate(vocab, tokenizer, prompt=test[1], num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five - match ODI series . India lost an international match under Rohit Sharma ' s captaincy after 12 consecutive victories dating back to March 2018 . The match witnessed India getting all out for 92 , their seventh lowest total in ODI cricket history . \n",
      "\n",
      "Headline:  India ' s top Test cricket team to score 100 - 1 win in ODI cricket ? \n"
     ]
    }
   ],
   "source": [
    "generate(vocab, tokenizer, prompt=test[2], num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  With Aegon Life iTerm Insurance plan , customers can enjoy tax benefits on your premiums paid and save up to â¹46 , 800 ^ on taxes . The plan provides life cover up to the age of 100 years . Also , customers have options to insure against Critical Illnesses , Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years . \n",
      "\n",
      "Headline:  WhatsAppi , AccelsApp to be used to â¹3 , 600 crore \n"
     ]
    }
   ],
   "source": [
    "generate(vocab, tokenizer, prompt=test[3], num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Speaking about the sexual harassment allegations against Rajkumar Hirani , Sonam Kapoor said , \" I ' ve known Hirani for many years . . . What if it ' s not true , the [ # MeToo ] movement will get derailed . \" \" In the # MeToo movement , I always believe a woman . But in this case , we need to reserve our judgment , \" she added . Hirani has been accused by an assistant who worked in ' Sanju ' . \n",
      "\n",
      "Headline:  When I don ' t have been an event to play : Shahidu \n"
     ]
    }
   ],
   "source": [
    "generate(vocab, tokenizer, prompt=test[4], num_samples=1, steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
