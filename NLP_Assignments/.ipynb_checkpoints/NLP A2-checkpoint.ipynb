{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6623e1da-a3bf-4a3f-bd60-6ed66f7bf68d",
   "metadata": {},
   "source": [
    "# NMSU CSCI-5435 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbe74d-3974-421d-8cbc-2238e4d07d5c",
   "metadata": {},
   "source": [
    "## Relevent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec545ff1-30e9-49ce-88e6-c662c06e42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name:               Tianjie Chen\n",
    "#Email:              tvc5586@nmsu.edu\n",
    "#File Creation Date: Feb/7/2025\n",
    "#Purpose of File:    NMSU CSCI-5435 Assignment 2\n",
    "#Last Edit Date:     Feb/7/2025\n",
    "#Last Edit Note:     File creation\n",
    "#GenAI used:         False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4329f57-32e4-43fb-b552-09cd62042db1",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6881497-77db-4f68-9a2d-3b179c651a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d5dfe-8af4-40a5-9586-1ad651996b36",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d73881-5613-4938-9b5e-756a4c4f930e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c288dd90-e0b1-4df6-984e-7b3fcb7b0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yelp_labelled.txt\", sep = '\\t', names=[\"Sentence\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b03252d-1b3c-4259-ac7d-efdbc3114cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = list(df.iloc[:, 0]), list(df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886198a-58e9-45c1-87ae-49ff30f0ad97",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf810ca-219b-40b2-b075-72c9c6d6a6e1",
   "metadata": {},
   "source": [
    "Referenced the [Remove punctuation Tutorial](https://www.geeksforgeeks.org/python-remove-punctuation-from-string/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f829d4f5-7afc-4982-bd7b-27b75f194f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = X[i].translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07609143-0e60-4b76-a0ed-6ae38d8dac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a74ce-4154-4caa-a413-a54dd860d19a",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceec238-530e-4857-9fb3-1ec5d3038bbc",
   "metadata": {},
   "source": [
    "Referenced the [Remove Stopwords Tutorial](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de61473e-9587-453c-99c1-fe8cf77299a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer\n",
    "word_vectorizer = CountVectorizer(stop_words = list(set(stopwords.words('english'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "109b71d0-ab65-40d2-99a6-f054cc33d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "word_train = word_vectorizer.fit_transform(X_train)\n",
    "word_test  = word_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f0962-5f88-495b-811f-8e0a931ce36a",
   "metadata": {},
   "source": [
    "### Sub-word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "990295e1-ba35-4243-862e-1993e302665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "old_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d3180a6-abba-448a-8ed0-f7893210d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "train_tokens = []\n",
    "test_tokens = []\n",
    "\n",
    "for i in X_train:\n",
    "    train_tokens.append(old_tokenizer.tokenize(i))\n",
    "\n",
    "for j in X_test:\n",
    "    test_tokens.append(old_tokenizer.tokenize(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ea8baea-5edd-4f08-87a1-77d785371987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As demosntrated during Feb 6 class\n",
    "def dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd8778e4-31cb-43fe-b5ac-4bde98fb3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer\n",
    "subword_vectorizer = CountVectorizer(\n",
    "    tokenizer = dummy, \n",
    "    preprocessor = dummy, \n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "214feb03-8303-4454-8403-243cd3db48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchen/.NLP_venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tchen/.NLP_venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "subword_train = subword_vectorizer.fit_transform(train_tokens)\n",
    "subword_test  = subword_vectorizer.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31928d-7b18-4d46-9053-5961b696e307",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24b536-d515-4c7d-b1e2-75c848d01e56",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dd9c4f2-08ce-4e02-b7c3-a0b18d9448d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6485bc7c-746d-495a-ae46-9e8338c7a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_word = MultinomialNB().fit(word_train.toarray(), y_train)\n",
    "NB_subword = MultinomialNB().fit(subword_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732d425-3c16-49f1-a34d-33062a4a72bc",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47ed0393-32ba-4be4-975d-55df2e494453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d76fba2-9505-4f79-ba01-856b7f69bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_word = LogisticRegression().fit(word_train.toarray(), y_train)\n",
    "LR_subword = LogisticRegression().fit(subword_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ddac4-a03b-4f47-8f3e-216fad6711a2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a88e98d5-9662-4e33-8910-669329a6ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8031aa9-563c-4a68-abf5-2db5055a2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get score\n",
    "NB_word_acc    = accuracy_score(y_test, NB_word.predict(word_test))\n",
    "NB_subword_acc = accuracy_score(y_test, NB_subword.predict(subword_test))\n",
    "LR_word_acc    = accuracy_score(y_test, LR_word.predict(word_test))\n",
    "LR_subword_acc = accuracy_score(y_test, LR_subword.predict(subword_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebda5e24-77bb-4412-a0be-0e55f634a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Navie Bayes Accuracy on word-based tokens: 0.79\n",
      "    Navie Bayes Accuracy on subword-based tokens: 0.725\n",
      "    Logistic Regression Accuracy on word-based tokens: 0.79\n",
      "    Logistic Regression Accuracy on subword-based tokens: 0.75\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Show score\n",
    "print(f\"\"\"\n",
    "    Navie Bayes Accuracy on word-based tokens: {NB_word_acc}\n",
    "    Navie Bayes Accuracy on subword-based tokens: {NB_subword_acc}\n",
    "    Logistic Regression Accuracy on word-based tokens: {LR_word_acc}\n",
    "    Logistic Regression Accuracy on subword-based tokens: {LR_subword_acc}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2daea2c-f0c8-48a5-9f94-248fd70ab92e",
   "metadata": {},
   "source": [
    "Based on these scores, word tokenization performs better than sub-word tokenization on this yelp dataset, regardless of the classifier chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5cb6f-b75f-4e86-8a96-ae245a55b0f5",
   "metadata": {},
   "source": [
    "## Export Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed0f6695-195b-4a3e-ad47-3a05e5354368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine datasets\n",
    "train_set, test_set = [], []\n",
    "\n",
    "for i, j in zip(X_train, y_train):\n",
    "    train_set.append([i, j])\n",
    "    \n",
    "for i, j in zip(X_test, y_test):\n",
    "    test_set.append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "121d43ea-08dd-459c-a284-66b78e8f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to txt files\n",
    "pd.DataFrame(train_set, columns = ['sentence', 'score']).to_csv(\"yelp_train.txt\", index = False, header = True)\n",
    "pd.DataFrame(test_set, columns = ['sentence', 'score']).to_csv(\"yelp_test.txt\", index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
