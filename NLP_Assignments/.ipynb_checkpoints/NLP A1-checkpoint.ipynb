{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e58fd-0f61-4dd3-9b85-4ea2711102d3",
   "metadata": {},
   "source": [
    "# NMSU CSCI-5435 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54e205-1056-45f3-bbd7-13e0c7cd6c7e",
   "metadata": {},
   "source": [
    "## Relevent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7737e6-0b66-410d-953a-110e5ed673a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name:               Tianjie Chen\n",
    "#Email:              tvc5586@nmsu.edu\n",
    "#File Creation Date: Jan/30/2025\n",
    "#Purpose of File:    NMSU CSCI-5435 Assignment 1\n",
    "#Last Edit Date:     Jan/30/2025\n",
    "#Last Edit Note:     Adding comments\n",
    "#GenAI used:         False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372b5d8-7350-4370-b882-939841abf246",
   "metadata": {},
   "source": [
    "## Setting up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25aee8-d47b-497c-ae5c-9c3922325573",
   "metadata": {},
   "source": [
    "### Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0238100f-d0f9-490b-a7cc-492e012b3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcec4672ce2408e9e674b703b0515c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c914d1d-6ad3-4d9c-a546-1630e3a4d404",
   "metadata": {},
   "source": [
    "## Assignment Task 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225858f7-cf0c-4bff-878c-672a5b65cf11",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4f3a9e-a79a-4831-a236-347144d1a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since yelp_review_full couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'yelp_review_full' at /home/tchen/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/0.0.0/c1f9ee939b7d05667af864ee1cb066393154bf85 (last modified on Thu Jan 30 07:59:43 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4588369-a9d3-41a8-a8b1-35df003a3e15",
   "metadata": {},
   "source": [
    "### Set up training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b91835-a137-4b6a-b9ac-1623aa6ef414",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"text\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8585eaa-aed9-4343-a839-75a8c0786380",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d27f4c1-80f7-4d8c-9f2f-04a13c77c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "old_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673aabb-afe2-41fd-99a2-c95f37fb1f8b",
   "metadata": {},
   "source": [
    "### Test untrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b624c874-e4d5-4bad-a749-8b9bb41a5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I miss this place. The grilled cheese was top notch. The fries were good and the vanilla shake was yummy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a96546-7cc3-436c-8d58-8df980769842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'miss', 'this', 'place', '.', 'The', 'g', '##rille', '##d', 'cheese', 'was', 'top', 'notch', '.', 'The', 'f', '##ries', 'were', 'good', 'and', 'the', 'van', '##illa', 'shake', 'was', 'y', '##um', '##my', '.']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b8cd9-d20c-4ea9-8d1e-e876f62e7879",
   "metadata": {},
   "source": [
    "### Train tokenizer on loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ebb638-cdf3-46b9-996e-ada570ca73f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba450e-0ec5-4eb5-a9ba-e89a5dc5449b",
   "metadata": {},
   "source": [
    "### Test trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49650740-5ee0-4b46-840e-be1f9f721465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'miss', 'this', 'place', '.', 'The', 'grilled', 'cheese', 'was', 'top', 'notch', '.', 'The', 'fries', 'were', 'good', 'and', 'the', 'vanilla', 'shake', 'was', 'yummy', '.']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce2de5-fe43-4556-99a2-ca2009c6817e",
   "metadata": {},
   "source": [
    "## Assignment Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933140d-e9b8-44dd-916b-decdf0edeb78",
   "metadata": {},
   "source": [
    "### Create the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b86a50-461f-4384-b200-a9a9e4b1b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from Levenshtein import distance\n",
    "\n",
    "\"\"\"\n",
    "A function that generates the most likely words\n",
    "based on the provided input token and vocabulary\n",
    "\n",
    "Parameters:\n",
    "    in_string : string\n",
    "        The input token\n",
    "    vocab : dict\n",
    "        The vocabulary\n",
    "\n",
    "Return:\n",
    "    suggestions : list (2D)\n",
    "        A list that contains the top 5 most likely\n",
    "        words from the vocabulary and their\n",
    "        corresponding edit distances. Each word and \n",
    "        its edit distance is grouped into a list\n",
    "\"\"\"\n",
    "def get_spelling_suggestions(in_string, vocab):\n",
    "    suggestions = [\n",
    "        [\"\", sys.maxsize],\n",
    "        [\"\", sys.maxsize],\n",
    "        [\"\", sys.maxsize],\n",
    "        [\"\", sys.maxsize],\n",
    "        [\"\", sys.maxsize],\n",
    "    ]\n",
    "\n",
    "    # for each token in the vocabulary\n",
    "    for token, _ in vocab.items():\n",
    "        # for each token-edit distance pair in the suggestions\n",
    "        for i in suggestions:\n",
    "            # Using levenshtein distance\n",
    "            edit_distance = distance(in_string, token, weights=(1, 1, 2))\n",
    "\n",
    "            # if token is already in the suggestions\n",
    "            if token == i[0]:\n",
    "                break\n",
    "\n",
    "            # if token's edit distance is less than what's in \n",
    "            # the suggestions\n",
    "            elif edit_distance < i[1]:\n",
    "                i[0] = token\n",
    "                i[1] = edit_distance\n",
    "                break\n",
    "\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc0d21-f6f5-4dce-bd4c-7f360bf9792d",
   "metadata": {},
   "source": [
    "### Create helper function for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347eeaf6-e014-448d-a737-feb492aeb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A helper function for printing the results from \n",
    "the main function\n",
    "\n",
    "Parameters:\n",
    "    in_string : string\n",
    "        The same input token provided to the \n",
    "        main function\n",
    "    suggestions : list (2D)\n",
    "        The suggestions provided by the main \n",
    "        function\n",
    "\"\"\"\n",
    "def print_suggestions(in_string, suggestions):\n",
    "    print(f\"The top 5 suggestions for correcting {in_string} are:\")\n",
    "    \n",
    "    for suggestion, edit_distance in suggestions:\n",
    "        print(f\"{suggestion} with edit distance of {edit_distance}\")\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5836192-af1a-45a8-8b50-383f060303a0",
   "metadata": {},
   "source": [
    "### Test the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000ec1fd-20a5-49b7-80ef-ba04ce517f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 suggestions for correcting luckilq are:\n",
      "luckily with edit distance of 2\n",
      "luck with edit distance of 3\n",
      "kil with edit distance of 4\n",
      "luc with edit distance of 4\n",
      "lucky with edit distance of 4\n",
      "\n",
      "The top 5 suggestions for correcting proffes are:\n",
      "profess with edit distance of 2\n",
      "profiles with edit distance of 3\n",
      "pres with edit distance of 3\n",
      "offs with edit distance of 3\n",
      "pros with edit distance of 3\n",
      "\n",
      "The top 5 suggestions for correcting skyises are:\n",
      "skies with edit distance of 2\n",
      "skis with edit distance of 3\n",
      "kiss with edit distance of 3\n",
      "skys with edit distance of 3\n",
      "sites with edit distance of 4\n",
      "\n",
      "The top 5 suggestions for correcting dismayo are:\n",
      "dismay with edit distance of 1\n",
      "mayo with edit distance of 3\n",
      "dism with edit distance of 3\n",
      "dismayed with edit distance of 3\n",
      "may with edit distance of 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"luckilq\", \"proffes\", \"skyises\", \"dismayo\"]\n",
    "\n",
    "# test each word in the test_words list\n",
    "for word in test_words:\n",
    "    print_suggestions(word, get_spelling_suggestions(word, tokenizer.vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
