{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236a4b50-a18e-48e9-a8fb-98ae9e13e357",
   "metadata": {
    "id": "236a4b50-a18e-48e9-a8fb-98ae9e13e357"
   },
   "source": [
    "# NMSU CSCI-5435 Assignment 4 Task 1 - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf641486-7003-4281-b689-10c285ec16b2",
   "metadata": {
    "id": "cf641486-7003-4281-b689-10c285ec16b2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Relevent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d78fbf-9b3f-427c-9c4e-616d1cbd4adf",
   "metadata": {
    "id": "30d78fbf-9b3f-427c-9c4e-616d1cbd4adf"
   },
   "outputs": [],
   "source": [
    "#Name:               Tianjie Chen\n",
    "#Email:              tvc5586@nmsu.edu\n",
    "#File Creation Date: Mar/17/2025\n",
    "#Purpose of File:    NMSU CSCI-5435 Assignment 4 Task 1\n",
    "#Last Edit Date:     Mar/17/2025\n",
    "#Last Edit Note:     File creation\n",
    "#GenAI used:         False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a9c38-d188-4c4d-bd44-87dd109414ac",
   "metadata": {
    "id": "202a9c38-d188-4c4d-bd44-87dd109414ac"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9311fc4-09f2-47b9-b27b-e21ce5d4d770",
   "metadata": {
    "id": "a9311fc4-09f2-47b9-b27b-e21ce5d4d770"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b43a1e-4e14-41fc-80b4-88fd0c4c2f01",
   "metadata": {
    "id": "c9b43a1e-4e14-41fc-80b4-88fd0c4c2f01"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf531a7-9116-49bb-b2b7-70ade9cdc50a",
   "metadata": {
    "id": "0bf531a7-9116-49bb-b2b7-70ade9cdc50a",
    "outputId": "e364577e-2661-414a-d511-2d99b8b55eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# USING GPU\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21efb022-be64-4829-9a30-9addfe342c06",
   "metadata": {
    "id": "21efb022-be64-4829-9a30-9addfe342c06"
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # BATCH SIZE FOR THIS MODEL\n",
    "epochs     = 5   # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4b7edd-8dbc-4eb6-af86-75f5a5ec10f3",
   "metadata": {
    "id": "8d4b7edd-8dbc-4eb6-af86-75f5a5ec10f3"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"News_Category_Dataset_v2.json\"\n",
    "\n",
    "train_data = pd.read_json(DATA_PATH, lines = True)['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7d072-0889-4a0b-97a6-073f09df31d9",
   "metadata": {
    "id": "82c7d072-0889-4a0b-97a6-073f09df31d9"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d011f2-9932-45f5-82a1-dae3472e30e0",
   "metadata": {
    "id": "91d011f2-9932-45f5-82a1-dae3472e30e0"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.str.lower()\n",
    "train_data = train_data.str.replace(\"-\", \" \", regex=True)\n",
    "train_data = train_data.str.replace(r\"[^'\\&\\w\\s]\", \"\", regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "train_data = [\" \".join([\"<start>\", x, \"<end>\"]) for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36651550-0199-47f2-8fd7-376295e97a60",
   "metadata": {
    "id": "36651550-0199-47f2-8fd7-376295e97a60",
    "outputId": "5aef7154-90cd-4c43-89a6-50f7ad72f7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> she left her husband he killed their children just another day in america <end>\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o-pxSfzQQYcK",
   "metadata": {
    "id": "o-pxSfzQQYcK"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5befc76c-700f-4d65-a92f-abc42dadeba6",
   "metadata": {
    "id": "5befc76c-700f-4d65-a92f-abc42dadeba6"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "###\n",
    "# define Vocab\n",
    "###\n",
    "class Vocab:\n",
    "    def __init__(self, list_of_sentence, tokenization, special_token, max_tokens=None):\n",
    "        # count vocab frequency\n",
    "        vocab_freq = {}\n",
    "        tokens = tokenization(list_of_sentence)\n",
    "        for t in tokens:\n",
    "            for vocab in t:\n",
    "                if vocab not in vocab_freq:\n",
    "                    vocab_freq[vocab] = 0\n",
    "                vocab_freq[vocab] += 1\n",
    "        # sort by frequency\n",
    "        vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda i: i[1], reverse=True)}\n",
    "        # create vocab list\n",
    "        self.vocabs = special_token + list(vocab_freq.keys())\n",
    "        if max_tokens:\n",
    "            self.vocabs = self.vocabs[:max_tokens]\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def _get_tokens(self, list_of_sentence):\n",
    "        for sentence in list_of_sentence:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            yield tokens\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.vocabs\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def append_token(self, token):\n",
    "        self.vocabs.append(token)\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def __call__(self, list_of_tokens):\n",
    "        def get_token_index(token):\n",
    "            if token in self.stoi:\n",
    "                return self.stoi[token]\n",
    "            else:\n",
    "                return 0\n",
    "        return [get_token_index(t) for t in list_of_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabs)\n",
    "\n",
    "###\n",
    "# generate Vocab\n",
    "###\n",
    "max_word = 50000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Must manually add the start and end tokens, otherwise\n",
    "# the tokenizer will separate them into three tokens\n",
    "tokenizer.add_tokens([\"<start>\", \"<end>\"])\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = Vocab(\n",
    "    train_data,\n",
    "    tokenization=yield_tokens,\n",
    "    special_token=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "\n",
    "# get list for index-to-word, and word-to-index.\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()\n",
    "\n",
    "# Add <pad> token\n",
    "vocab.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MXK7CU9YQYcN",
   "metadata": {
    "id": "MXK7CU9YQYcN"
   },
   "source": [
    "#### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9WS2o247QYcP",
   "metadata": {
    "id": "9WS2o247QYcP"
   },
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim, rnn_units, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_units,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classify = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, states=None, return_final_state=False):\n",
    "        # embedding\n",
    "        #   --> (batch_size, seq_len, embedding_dim)\n",
    "        outs = self.embedding(inputs)\n",
    "        # build \"lengths\" property to pack inputs (see above)\n",
    "        lengths = (inputs != self.padding_idx).int().sum(dim=1, keepdim=False)\n",
    "        # pack inputs for RNN\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            outs,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        # apply RNN\n",
    "        if states is None:\n",
    "            packed_outs, final_state = self.LSTM(packed_inputs)\n",
    "        else:\n",
    "            packed_outs, final_state = self.LSTM(packed_inputs, states)\n",
    "        # unpack results\n",
    "        #   --> (batch_size, seq_len, rnn_units)\n",
    "        outs, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0,\n",
    "            total_length=self.seq_len,\n",
    "        )\n",
    "        # apply feed-forward to classify\n",
    "        #   --> (batch_size, seq_len, vocab_size)\n",
    "        logits = self.classify(outs)\n",
    "        # return results\n",
    "        if return_final_state:\n",
    "            return logits, final_state  # This is used in prediction\n",
    "        else:\n",
    "            return logits               # This is used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pAY2TWzSQYcR",
   "metadata": {
    "id": "pAY2TWzSQYcR"
   },
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-SsOlFJXQYcT",
   "metadata": {
    "id": "-SsOlFJXQYcT"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "rnn_units = 512\n",
    "max_seq_len = 256\n",
    "pad_index = vocab.__len__() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "AeeuNeQAQYcU",
   "metadata": {
    "id": "AeeuNeQAQYcU"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, feature_list = [], []\n",
    "    for text in batch:\n",
    "        # tokenize to a list of word's indices\n",
    "        tokens = vocab(tokenizer.tokenize(text))\n",
    "        # separate into features and labels\n",
    "        y = tokens[1:]\n",
    "        y.append(-100)\n",
    "        x = tokens\n",
    "        # limit length to max_seq_len\n",
    "        y = y[:max_seq_len]\n",
    "        x = x[:max_seq_len]\n",
    "        # pad features and labels\n",
    "        y += [-100] * (max_seq_len - len(y))\n",
    "        x += [pad_index] * (max_seq_len - len(x))\n",
    "        # add to list\n",
    "        label_list.append(y)\n",
    "        feature_list.append(x)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    feature_list = torch.tensor(feature_list, dtype=torch.int64).to(device)\n",
    "    return label_list, feature_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Hh683-FiQYcU",
   "metadata": {
    "id": "Hh683-FiQYcU"
   },
   "outputs": [],
   "source": [
    "model = LSTM_LM(\n",
    "    vocab_size=vocab.__len__(),\n",
    "    seq_len=max_seq_len,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    padding_idx=pad_index).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Ry9t3c-jQYcV",
   "metadata": {
    "id": "Ry9t3c-jQYcV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 5.6789 - accuracy: 0.1353\n",
      "Epoch 2 - loss: 5.2175 - accuracy: 0.2144\n",
      "Epoch 3 - loss: 4.7239 - accuracy: 0.2272\n",
      "Epoch 4 - loss: 4.9198 - accuracy: 0.1968\n",
      "Epoch 5 - loss: 4.8169 - accuracy: 0.2237\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for labels, seqs in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(seqs.to(device))\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=2)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        num_total = (labels != -100).float().sum()\n",
    "        accuracy = num_correct / num_total\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OI4VONtqQYcW",
   "metadata": {
    "id": "OI4VONtqQYcW"
   },
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "qUkA0hpr8bz_",
   "metadata": {
    "id": "qUkA0hpr8bz_"
   },
   "outputs": [],
   "source": [
    "end_index = stoi[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def pred_output(text):\n",
    "    generated_text = \"<start> \" + text\n",
    "    _, inputs = collate_batch([generated_text])\n",
    "    mask = (inputs != pad_index).int()\n",
    "    last_idx = mask[0].sum() - 1\n",
    "    final_states = None\n",
    "    outputs, final_states = model(inputs, final_states, return_final_state=True)\n",
    "    pred_index = outputs[0][last_idx].argmax()\n",
    "    for loop in range(max_output):\n",
    "        generated_text += \" \"\n",
    "        next_word = itos[pred_index]\n",
    "        generated_text += next_word\n",
    "        if pred_index.item() == end_index:\n",
    "            break\n",
    "        _, inputs = collate_batch([next_word])\n",
    "        outputs, final_states = model(inputs, final_states, return_final_state=True)\n",
    "        pred_index = outputs[0][0].argmax()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "GArwJsIwQYcX",
   "metadata": {
    "id": "GArwJsIwQYcX",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> in the united states president o ##ba ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es ##s ##es\n",
      "<start> the man has accused by the us president elect <end>\n",
      "<start> now he was expected to be a woman who has been a long history of the <end>\n"
     ]
    }
   ],
   "source": [
    "print(pred_output(\"in the united states president\"))\n",
    "print(pred_output(\"the man has accused by\"))\n",
    "print(pred_output(\"now he was expected to\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cf641486-7003-4281-b689-10c285ec16b2",
    "202a9c38-d188-4c4d-bd44-87dd109414ac",
    "c9b43a1e-4e14-41fc-80b4-88fd0c4c2f01",
    "82c7d072-0889-4a0b-97a6-073f09df31d9",
    "6b493dfb-cd13-4be6-92a1-3242b37eab77"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
