{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2651cc74-57f0-48d1-851c-2205c27e4660",
   "metadata": {},
   "source": [
    "# NMSU CSCI-5435 Assignment 6 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffddda-1dc9-41f9-8532-211d2d0faa6c",
   "metadata": {},
   "source": [
    "## Relevent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b0cc8-f83b-4df0-a5ca-db1591afb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name:               Tianjie Chen\n",
    "#Email:              tvc5586@nmsu.edu\n",
    "#File Creation Date: Apr/23/2025\n",
    "#Purpose of File:    NMSU CSCI-5435 Assignment 6 Task 2\n",
    "#Last Edit Date:     Apr/23/2025\n",
    "#Last Edit Note:     File creation\n",
    "#GenAI used:         False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cc06d-9f83-4715-b1be-af859968dbf0",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca6f90b-256f-48c5-bf74-658b7dd1e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import nltk.data\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e63ef-91ca-4d36-9e52-78cbc2d9af01",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644ea929-dd66-4b4b-b8ec-d4a800630cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# USING GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc852bf-1238-45ea-bb0e-f4ad76a74231",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"eng_-french.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10898d3-48fa-4b05-95e5-0d063c7914a3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7fdc1f-bc34-455b-af8a-ddddb549b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)[:,[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b06a0c-d056-4aef-a91c-8a5e2283036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't say a word to anyone.\" 'Ne dis mot à personne.']\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9721cb59-a7eb-4502-a337-56a4b0ea90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/tchen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "tokenizer_en = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "tokenizer_fr = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "\n",
    "en_list = []\n",
    "fr_list = []\n",
    "\n",
    "for x in data:\n",
    "    x1 = tokenizer_en.tokenize(x[0])\n",
    "    x2 = tokenizer_fr.tokenize(x[1])\n",
    "    if len(x1) == len(x2):\n",
    "        en_list += x1\n",
    "        fr_list += x2\n",
    "\n",
    "data = np.column_stack((en_list, fr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4771cb1-f42a-490d-9282-06327296376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.char.lower(data)\n",
    "data = np.char.replace(data, \"-\", \" \")\n",
    "for x in string.punctuation.replace(\"'\", \"\"):\n",
    "    data = np.char.replace(data, x, \"\")\n",
    "for x in \"«»\":\n",
    "    train_data = np.char.replace(data, x, \"\")\n",
    "data = np.char.strip(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dc9912-ff6d-4282-be5b-ec43001c999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data = data.rename(columns={0: \"English\", 1: \"French\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d320376-eb47-48c6-9c42-b51d88c6bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    data, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22685bb-812a-4c3d-a194-9b964ad1749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a5bad4-259e-4992-809c-032183e3ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"translate English to French: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"English\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=100, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=list(examples[\"French\"]), max_length=100, truncation=True)\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        _ = {\"text\": inputs[i], \"input_ids\": model_inputs[i].ids, \"labels\": labels[i].ids}\n",
    "        processed_data.append(_)\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e982689b-7e55-4680-9358-9efa7bbf2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = preprocess_function(train)\n",
    "tokenized_test  = preprocess_function(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9958bc4-b0f8-426f-82f9-0b5138d2a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'translate English to French: you can only use it once', 'input_ids': [13959, 1566, 12, 2379, 10, 25, 54, 163, 169, 34, 728, 1], 'labels': [3, 17, 76, 3, 29, 15, 3, 16162, 3, 40, 31, 16578, 546, 31, 444, 2529, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9b4da6-e149-401a-a6de-92f9bae7a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabae4d-fd21-45f8-a5b4-e060f0bef9ab",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12dc8026-0f56-4138-8f9e-21f4088a9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33060a56-4764-40d4-8c02-de5c25bf0d14",
   "metadata": {},
   "source": [
    "## Define & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2eaa3f3-0fba-43f6-aa09-532ddb9e773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362d5a84-bef7-4aa1-8930-09775b1a944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60998/360464422.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4412' max='4412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4412/4412 14:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.744710</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>12.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>0.706512</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>12.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.859500</td>\n",
       "      <td>0.690237</td>\n",
       "      <td>0.673600</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>12.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.685822</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>12.071700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4412, training_loss=0.8985115321030535, metrics={'train_runtime': 885.9131, 'train_samples_per_second': 637.354, 'train_steps_per_second': 4.98, 'total_flos': 3922106015023104.0, 'train_loss': 0.8985115321030535, 'epoch': 4.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"NLP_A6\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2e71d-f294-475f-8a9c-50ba95dca54f",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17baa6a8-00a7-4899-90d0-1a99ff6b9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: I have no idea if this sentence is translated correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1c661c-674a-4940-b7c5-7290ef6ebfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "894f4745-93ad-46fa-a533-1cbea0711c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7b60bc3-6764-467b-a8f9-9a1024b332dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"je n'ai aucune idée si cette phrase est traduite correctement.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
